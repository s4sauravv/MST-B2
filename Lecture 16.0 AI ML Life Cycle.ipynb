{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab9a4a82",
   "metadata": {},
   "source": [
    "# AI/ML Life Cycle "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f846bbde",
   "metadata": {},
   "source": [
    " 1 -  Start\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " 2 -  Data Selection - Data will be come from different different sources . it can be DataBase ,Server, Web site , files. and some time CSV File.\n",
    " \n",
    " \n",
    " \n",
    " 3 - Data Description - Then , once we read the data , you will be keep in tha dataframe then you check statistics  and story about  the dataset . Like How many features present.\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " 4 - Performing Both Statistical and Graphical Data Analysis -  We Know EDA(Exploratory Data Analysis) part, statistical analysis checking our data is distributed normally . checking outliers , right skewed , left skewed.\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " 5 - Data Transformation and Derivation of new attributes if necessary  -  We Just did what data transformation encoding . if there is a need to create a new column we will do that.\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " 6- Selection of Machine Learning algorithms Based on the patterns observed in EDA - You have seen features and labels to do this analysis and all looking in the dataframe we understand that wheather it is regression problem or classification problem based on what we observe within a regression we have lots of algorithm and classification also we have more algorithms and there is a binary classification or multi class feature.\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " 7 -  Data Standardization and Normalization  -   Data standardization we did simply using standard scaler , normalization is very similer to standard scaler.\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " 8 - Creation of train and test data set using optimum parameters  - We have to split into train and test data set. using Optimum Parameter .\n",
    " \n",
    " \n",
    " Optimum Parameter -> Test Size , Random State.\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "9 - Model Training using the Machine Learning algorithm tested above - After that we started model training by passing x_train,y_train.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "10 - Calculation of Model Accuracy Both Training and test Accuracy - Both Training and test accuracies what is the training score and testing score .\n",
    "\n",
    "\n",
    "- If we are not satified with result.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "11 -  Hyperparameter tuning to achieve better accuracy - If we are not satisfied with the result we try to tune the parameter . that is hyperparameter tuning . to achieve the better accuracy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "12 - Saving  the created model file - Once we satisfied with the accuracy then we save our model by using pickle.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "13 - Deployment Strategies For The model (Live/Stream/Batch/Mini Batch) - We have to move into production. where real time prediction will be done . So that we have to deploy our model or productionised our model . in deployment strategies when we should push into production we have to discuss with a client .\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "14 - Production Deployment and Testing  - as soon as we move again we are going to testing earlier we are tested in our system now we are going to test live . Checking Prediction accuracy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "15 - Finalizing the Retraining approach - When you build a model or job is not done . you did the model building push into production so that is not the final model . For ever you are not going to use  same model . Why ? Because your data is keep changing basically you have to trained new set of data .\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "16 - Logging and Monitoring (Maintaining the audit tables) - This will taken care by support team or production support . they only made for support only they checking job are running it is producing same result or not they will moniter it . and they will document using tools , and some files and some software . next day manager and Data Scientist will check there is any deviation is there is any problem with a data accuracy. data descriptency and all.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "17 -  DashBoard for monitoring and Logging Reports  -   Whatever is monitered here logged here you have to send report to clients okay . Today this is how the data frequency , this is how the data frequency. this is the accuracy how this is behaving . writing in a mail generate some reports . Some dashboards usually this will be taken care by some tableu people. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "18 - STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd8b93c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Engineer - Saurav\n",
    "#Date - 18-5-2022\n",
    "#Time - 10:02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f98cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
